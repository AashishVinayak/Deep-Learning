{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_captioning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "7TCmGeWhcpix",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install gensim\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gensim\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "import requests, zipfile, io\n",
        "from PIL import Image\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.layers import Dense, Flatten"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kHabxZ_NjKbS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# downloading data\n",
        "image_data_url = \"http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/Flickr8k_Dataset.zip\"\n",
        "text_data_url = \"http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/Flickr8k_text.zip\"\n",
        "\n",
        "# downloading images\n",
        "images_from_url = requests.get(image_data_url)\n",
        "# extraction\n",
        "images_zip = zipfile.ZipFile(io.BytesIO(images_from_url.content))\n",
        "images_zip.extractall()\n",
        "\n",
        "# downloading captions\n",
        "text_from_url = requests.get(text_data_url)\n",
        "# extraction\n",
        "text_zip = zipfile.ZipFile(io.BytesIO(text_from_url.content))\n",
        "text_zip.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GfC8T4OQE8lM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "ac426ad1-a704-4753-acf1-ee51a002116b"
      },
      "cell_type": "code",
      "source": [
        "### Preparing metadata\n",
        "\n",
        "# respective captions\n",
        "captions = open(\"Flickr8k.token.txt\").read()\n",
        "\n",
        "# some processing magic\n",
        "captions = captions.split(\"\\n\")\n",
        "captions = [i.split(\"\\t\") for i in captions]\n",
        "\n",
        "# removing (.)\n",
        "for c in range(len(captions)-1):\n",
        "    captions[c][1] = ' '.join(re.findall(r'\\w+[a-zA-Z]', captions[c][1]))\n",
        "    \n",
        "\n",
        "# cleaning up the image names\n",
        "for j in range(len(captions)):\n",
        "    captions[j][0] = captions[j][0][0:len(captions[j][0])-2]\n",
        "\n",
        "    \n",
        "# this dictionary holds the image names and respective captions\n",
        "meta_data = {}\n",
        "for i in range(0, len(captions)-1, 5):\n",
        "    meta_data[captions[i][0]] = [captions[i+j][1] for j in range(5)]\n",
        "    \n",
        "# sample\n",
        "i = 0\n",
        "for m in meta_data.keys():\n",
        "    print(m, \":\")\n",
        "    print(meta_data[m], \"\\n\")\n",
        "    i+=1\n",
        "    if i==3:break\n",
        "\n",
        "# removing a bad key\n",
        "del(meta_data[\"2258277193_586949ec62.jpg.1\"])"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000268201_693b08cb0e.jpg :\n",
            "['child in pink dress is climbing up set of stairs in an entry way', 'girl going into wooden building', 'little girl climbing into wooden playhouse', 'little girl climbing the stairs to her playhouse', 'little girl in pink dress going into wooden cabin'] \n",
            "\n",
            "1001773457_577c3a7d70.jpg :\n",
            "['black dog and spotted dog are fighting', 'black dog and tri colored dog playing with each other on the road', 'black dog and white dog with brown spots are staring at each other in the street', 'Two dogs of different breeds looking at each other on the road', 'Two dogs on pavement moving toward each other'] \n",
            "\n",
            "1002674143_1b742ab4b8.jpg :\n",
            "['little girl covered in paint sits in front of painted rainbow with her hands in bowl', 'little girl is sitting in front of large painted rainbow', 'small girl in the grass plays with fingerpaints in front of white canvas with rainbow on it', 'There is girl with pigtails sitting in front of rainbow painting', 'Young girl with pigtails painting outside in the grass'] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IyRN_woC1Vcw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## hyperparameters\n",
        "em_vector_len = 200\n",
        "gru_cell_len = 512\n",
        "gru_cell_len_2 = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w52z8kS1a-BT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "f18e44b1-70d0-4e58-cb96-9a250fe75d56"
      },
      "cell_type": "code",
      "source": [
        "### making word embedding\n",
        "# to do this we'll loop over all the sentences in the whole dataset and make word vectors out of it using gensim\n",
        "\n",
        "# this list will hold all the sentences \n",
        "sentences = []\n",
        "\n",
        "for key in meta_data.keys():\n",
        "    for s in meta_data[key]:\n",
        "        sentences.append(s.split())\n",
        "        \n",
        "# training the word2vec model\n",
        "embedding = Word2Vec(sentences, size=em_vector_len, window=4, sg=1, workers=10)\n",
        "embedding.train(sentences, total_examples=len(sentences), epochs = 10)\n",
        "\n",
        "# demo\n",
        "embedding.wv.most_similar(\"black\", topn=3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('long-haired', 0.6281743049621582),\n",
              " ('fluffy', 0.6118836402893066),\n",
              " ('beige', 0.60596764087677)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "9lSXLItgzcKW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "### Weights \n",
        "w_1 = {'w_rx': tf.get_variable(\"w_rx_1\", [gru_cell_len, em_vector_len], initializer=tf.contrib.layers.xavier_initializer()),\n",
        "     'w_ra': tf.get_variable(\"w_ra_1\", [gru_cell_len, gru_cell_len], initializer=tf.contrib.layers.xavier_initializer()),\n",
        "     'w_ux': tf.get_variable(\"w_ux_1\", [gru_cell_len, em_vector_len], initializer=tf.contrib.layers.xavier_initializer()),\n",
        "     'w_ua': tf.get_variable(\"w_ua_1\", [gru_cell_len, gru_cell_len], initializer=tf.contrib.layers.xavier_initializer()),\n",
        "     'w_hx': tf.get_variable(\"w_hx_1\", [gru_cell_len, em_vector_len], initializer=tf.contrib.layers.xavier_initializer()),\n",
        "     'w_ar': tf.get_variable(\"w_ar_1\", [gru_cell_len, gru_cell_len], initializer=tf.contrib.layers.xavier_initializer())\n",
        "    }\n",
        "\n",
        "\n",
        "w_2 = {'w_rx': tf.get_variable(\"w_rx_2\", [gru_cell_len, gru_cell_len_2], initializer=tf.contrib.layers.xavier_initializer()),\n",
        "     'w_ra': tf.get_variable(\"w_ra_2\", [gru_cell_len, gru_cell_len], initializer=tf.contrib.layers.xavier_initializer()),\n",
        "     'w_ux': tf.get_variable(\"w_ux_2\", [gru_cell_len, gru_cell_len_2], initializer=tf.contrib.layers.xavier_initializer()),\n",
        "     'w_ua': tf.get_variable(\"w_ua_2\", [gru_cell_len, gru_cell_len], initializer=tf.contrib.layers.xavier_initializer()),\n",
        "     'w_hx': tf.get_variable(\"w_hx_2\", [gru_cell_len, gru_cell_len_2], initializer=tf.contrib.layers.xavier_initializer()),\n",
        "     'w_ar': tf.get_variable(\"w_ar_2\", [gru_cell_len, gru_cell_len], initializer=tf.contrib.layers.xavier_initializer())\n",
        "    }\n",
        "\n",
        "### Placeholders\n",
        "x = tf.placeholder(tf.float32, [None, em_vector_len])\n",
        "y = tf.placeholder(tf.float32, [None, em_vector_len])\n",
        "x_img = tf.placeholder(tf.float32, [None, 224, 224, 3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tQ-3ZL1xF81D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# using InceptionV3 for the image classifier(encoder)\n",
        "\n",
        "vgg = VGG16(include_top=False, weights='imagenet', input_tensor=x_img, input_shape=(224,224,3))\n",
        "encoder = vgg.output\n",
        "encoder = Dense(1024, activation=\"relu\")(encoder)\n",
        "encoder = Flatten()(encoder)\n",
        "encoder = Dense(512, activation=\"tanh\")(encoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1OdVz9i5hVqp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# GRU Cell\n",
        "def Gru_cell(n_h, a, x_in, weights, return_seq = False, y_hat = [], t=0):\n",
        "    \"\"\"\n",
        "        n_h        : number of neurons in hidden layer\n",
        "        a          : Initial hidden state\n",
        "        x_in       : \n",
        "        return_seq : whether to return all the values of prediction vector yhat or just the last output\n",
        "        t          : Time Step\n",
        "    \"\"\"\n",
        "    a = tf.reshape(a, (n_h, 1))\n",
        "   \n",
        "    x_calc = tf.reshape(x_in[t], (x_in[t].shape[0], 1))\n",
        "    \n",
        "    reset_gate = tf.nn.sigmoid(tf.matmul(weights['w_rx'], x_calc) + tf.matmul(weights['w_ra'], a))\n",
        "    update_gate = tf.nn.sigmoid(tf.matmul(weights['w_ux'], x_calc) + tf.matmul(weights['w_ua'], a))\n",
        "    \n",
        "    cell = tf.nn.tanh(tf.matmul(weights['w_hx'], x_calc) + tf.matmul(weights['w_ar'], tf.multiply(a, reset_gate)))\n",
        "    a = tf.multiply( (1-update_gate), cell) + tf.multiply(update_gate, a)\n",
        "    y_hat.append(tf.nn.softmax(a))\n",
        "    \n",
        "    \n",
        "    # recursively iterate over all time steps\n",
        "    if t == x_in[t].shape[0] - 1:\n",
        "        Gru_cell(n_h, a, x_in[t+1], return_seq, y_hat = y_hat, t = t+1)\n",
        "    \n",
        "    if return_seq == True:\n",
        "        return y_hat\n",
        "    else: \n",
        "        return y_hat[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rRVO9fBFUGLu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6818f8a1-bb6c-42ed-c56d-6720f85053c0"
      },
      "cell_type": "code",
      "source": [
        "# preparing image data\n",
        "# empty list to hold all the images\n",
        "images = []\n",
        "\n",
        "# loop over all keys in meta_data \n",
        "for img in meta_data.keys():\n",
        "    # image object -> numpy array -> append to list\n",
        "    images.append(np.asarray(Image.open(\"Flicker8k_Dataset/\" + str(img)).resize((224,224))).reshape(224,224,3))\n",
        "    \n",
        "images = np.array(images)  \n",
        "\n",
        "print(\"image data: \", images.shape)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image data:  (8091, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SPtvjvVGSKqY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# preparing text data\n",
        "# dictionary to hold word vectors for images\n",
        "text = {}\n",
        "\n",
        "for key in meta_data.keys():\n",
        "    for word in \n",
        "    text[key] = \n",
        "########################## fix this ##########################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ceRW0XVAa2s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "s = tf.Session()\n",
        "s.run(tf.global_variables_initializer())\n",
        "\n",
        "img = np.asarray(Image.open(\"Flicker8k_Dataset/1000268201_693b08cb0e.jpg\").resize((224,224))).reshape(1,224,224,3)\n",
        "\n",
        "encoder_vector = s.run(encoder, {x_img: img})\n",
        "\n",
        "\n",
        "decoder_1 = Gru_cell(gru_cell_len, e, x, weights = w_1, return_seq=True)\n",
        "decoder_2 = Gru_cell(gru_cell_len, e, decoder_1, weights = w_2, return_seq=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9C_965nQNs3y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"embedding.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"l5jxNiBHuuQ-","colab_type":"code","colab":{}},"cell_type":"code","source":["####  a sample code for word embedding\n","#!pip install gensim\n","import re\n","import nltk\n","import gensim\n","from gensim.models import Word2Vec\n","import numpy as np\n","\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bQerQw-2uyPP","colab_type":"code","colab":{}},"cell_type":"code","source":["# importing data\n","with open('text.txt',encoding=\"utf8\", errors='ignore') as file:\n","    data = file.read()\n","\n","# keeping only alphabets\n","data = re.findall(r'\\w+[a-zA-Z]', data.lower())\n","\n","# removing stopwords\n","stops = stopwords.words('english')\n","data = [w for w in data if w not in stops]\n","data = ' '.join(data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XfCAC3qau3Ch","colab_type":"code","colab":{}},"cell_type":"code","source":["# total words in one segment\n","window_size = 6\n","# inbuilt gensim preprocessing\n","data_processed = gensim.utils.simple_preprocess(data.lower())\n","# segmenting words i.e. \"window_size\" no. of words in one list\n","data_processed = [data_processed[i:i+window_size] for i in range(0, len(data_processed))]\n","\n","# skip_gram model in gensim\n","embedding = Word2Vec(data_processed, size=100, window=window_size, min_count=2, workers=10, sg=1)\n","# training\n","embedding_layer = embedding.train(data_processed, total_examples=len(data_processed), epochs=300)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KgK8_bMzdW0g","colab_type":"code","colab":{}},"cell_type":"code","source":["# dataset preparation\n","# holds input training data\n","in_data_vectors = np.zeros([len(data_processed), window_size-1, 100])\n","# holds output label data i.e. input[i] + 1 (next word)\n","out_data_vectors = np.zeros([len(data_processed), 1, 100])\n","\n","# storing word vector of each word\n","for i, sentence in enumerate(data_processed):\n","    for j in sentence:\n","        # vectors of first n-1 words will be stored in the input data, where n = window_size (eg: total 6 words, 5 in input, 1 in output)\n","        if j<(window_size-1):\n","            in_data_vectors[i][j] = embedding.wv.word_vec(sentence[j])\n","        else:\n","            out_data_vectors[i][0] = embedding.wv.word_vec(sentence[j])\n","            continue"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TTfGdUAM6D5H","colab_type":"code","colab":{}},"cell_type":"code","source":["# training and testing set\n","# no. of training examples at a given percentage\n","split = int(len(in_data_vectors)*0.8)\n","x_train = in_data_vectors[0:split]\n","y_train = out_data_vectors[0:split]\n","\n","x_test = in_data_vectors[split:len(out_data_vectors)]\n","y_test = out_data_vectors[split:len(out_data_vectors)]"],"execution_count":0,"outputs":[]}]}
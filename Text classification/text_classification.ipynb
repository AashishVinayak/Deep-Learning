{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text_classification.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"r_lwgMXVabNs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":90},"outputId":"73ac34a2-55b8-41e1-aa42-d682b9146d35","executionInfo":{"status":"ok","timestamp":1540878787012,"user_tz":-330,"elapsed":1675,"user":{"displayName":"Aashish Vinayak","photoUrl":"https://lh3.googleusercontent.com/-n6LvT5UqaRw/AAAAAAAAAAI/AAAAAAAAFxw/5OKvaxSzOmE/s64/photo.jpg","userId":"11492081867973563164"}}},"cell_type":"code","source":["#!pip install gensim\n","import pandas as pd\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","import numpy as np\n","import re\n","import tensorflow as tf\n","import gensim\n","from gensim.models import Word2Vec\n","from keras.models import Sequential\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","from nltk.corpus import stopwords\n","stopwords = list(set(stopwords.words(\"english\")))"],"execution_count":36,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"metadata":{"id":"9dTg4lYDhCHJ","colab_type":"code","colab":{}},"cell_type":"code","source":["def unique(sequence):\n","    \"\"\"\n","        function to remove duplicate values without losing the order\n","        http://www.martinbroadhurst.com/removing-duplicates-from-a-list-while-preserving-order-in-python.html\n","    \"\"\"\n","    seen = set()\n","    return [x for x in sequence if not (x in seen or seen.add(x))]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rToAmRRsNR_R","colab_type":"code","colab":{}},"cell_type":"code","source":["# one_hot maker\n","def one_hot(vector):\n","    one_hot_vec = np.zeros([len(vector), int(max(vector[0:len(vector)])+1)])\n","    \n","    for i in range(len(one_hot_vec)):\n","        one_hot_vec[i, vector[i, 0]] = 1\n","       \n","    return one_hot_vec"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qLJ9TiA8TS2f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":198},"outputId":"27a8f4c1-9bc2-4143-9600-d566cc4adee6","executionInfo":{"status":"ok","timestamp":1540880987297,"user_tz":-330,"elapsed":1734,"user":{"displayName":"Aashish Vinayak","photoUrl":"https://lh3.googleusercontent.com/-n6LvT5UqaRw/AAAAAAAAAAI/AAAAAAAAFxw/5OKvaxSzOmE/s64/photo.jpg","userId":"11492081867973563164"}}},"cell_type":"code","source":["# downloading data\n","url = \"https://raw.githubusercontent.com/aashishksahu/Deep-Learning/master/Text%20classification/Womens%20Clothing%20E-Commerce%20Reviews.csv\"\n","data = pd.DataFrame(pd.read_csv(url))\n","# since some reviews are empty, they're being dropped\n","data = data.dropna(axis=0)\n","#data = data.sample(frac=1)\n","data.head()"],"execution_count":83,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Review Text</th>\n","      <th>Rating</th>\n","      <th>Positive Feedback Count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Absolutely wonderful - silky and sexy and comf...</td>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n","      <td>5</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I had such high hopes for this dress and reall...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n","      <td>5</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>This shirt is very flattering to all due to th...</td>\n","      <td>5</td>\n","      <td>6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                         Review Text  Rating  \\\n","0  Absolutely wonderful - silky and sexy and comf...       4   \n","1  Love this dress!  it's sooo pretty.  i happene...       5   \n","2  I had such high hopes for this dress and reall...       3   \n","3  I love, love, love this jumpsuit. it's fun, fl...       5   \n","4  This shirt is very flattering to all due to th...       5   \n","\n","   Positive Feedback Count  \n","0                        0  \n","1                        4  \n","2                        0  \n","3                        0  \n","4                        6  "]},"metadata":{"tags":[]},"execution_count":83}]},{"metadata":{"id":"_pt3PbuJ0LPR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":419},"outputId":"c8b42cc0-7023-4a56-9663-d6ccaad40f59","executionInfo":{"status":"ok","timestamp":1540880998939,"user_tz":-330,"elapsed":8649,"user":{"displayName":"Aashish Vinayak","photoUrl":"https://lh3.googleusercontent.com/-n6LvT5UqaRw/AAAAAAAAAAI/AAAAAAAAFxw/5OKvaxSzOmE/s64/photo.jpg","userId":"11492081867973563164"}}},"cell_type":"code","source":["# processing the text\n","text = list(data['Review Text'])\n","\n","# sample\n","print(text[0])\n","print(text[3])\n","\n","print(\"\\n* After removing punctuations and full stops:\\n\")\n","text = [re.findall(r'\\w+[A-Za-z]', w.lower()) for w in text]\n","\n","print(text[0])\n","print(text[3])\n","\n","print(\"\\n* Removing stopwords: \\n\")\n","for i in range(len(text)):\n","    text[i] = [w for w in text[i] if not w in stopwords]\n","\n","print(text[0])\n","print(text[3])\n","\n","print(\"\\n* Lemmatizing: \\n\")\n","wnl = WordNetLemmatizer()\n","for i in range(len(text)):\n","    text[i] = [wnl.lemmatize(word) for word in text[i]]\n","\n","print(text[0])\n","print(text[3])\n","    \n","print(\"\\n* Removing duplicates\")\n","text = [unique(s) for s in text]\n","\n","print(text[0])\n","print(text[3])\n","\n","# finally replacing raw text with processed one\n","\n","text = [\" \".join(t) for t in text]"],"execution_count":84,"outputs":[{"output_type":"stream","text":["Absolutely wonderful - silky and sexy and comfortable\n","I love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments!\n","\n","* After removing punctuations and full stops:\n","\n","['absolutely', 'wonderful', 'silky', 'and', 'sexy', 'and', 'comfortable']\n","['love', 'love', 'love', 'this', 'jumpsuit', 'it', 'fun', 'flirty', 'and', 'fabulous', 'every', 'time', 'wear', 'it', 'get', 'nothing', 'but', 'great', 'compliments']\n","\n","* Removing stopwords: \n","\n","['absolutely', 'wonderful', 'silky', 'sexy', 'comfortable']\n","['love', 'love', 'love', 'jumpsuit', 'fun', 'flirty', 'fabulous', 'every', 'time', 'wear', 'get', 'nothing', 'great', 'compliments']\n","\n","* Lemmatizing: \n","\n","['absolutely', 'wonderful', 'silky', 'sexy', 'comfortable']\n","['love', 'love', 'love', 'jumpsuit', 'fun', 'flirty', 'fabulous', 'every', 'time', 'wear', 'get', 'nothing', 'great', 'compliment']\n","\n","* Removing duplicates\n","['absolutely', 'wonderful', 'silky', 'sexy', 'comfortable']\n","['love', 'jumpsuit', 'fun', 'flirty', 'fabulous', 'every', 'time', 'wear', 'get', 'nothing', 'great', 'compliment']\n"],"name":"stdout"}]},{"metadata":{"id":"L40cU7PP0q3C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"580252ad-5bcb-47e9-a4b4-1e86717eba9c","executionInfo":{"status":"ok","timestamp":1540878896033,"user_tz":-330,"elapsed":6405,"user":{"displayName":"Aashish Vinayak","photoUrl":"https://lh3.googleusercontent.com/-n6LvT5UqaRw/AAAAAAAAAAI/AAAAAAAAFxw/5OKvaxSzOmE/s64/photo.jpg","userId":"11492081867973563164"}}},"cell_type":"code","source":["\n","# converting text to TF-IDF score\n","# Using TF-IDF because in text classification we need the importance of\n","# special words that indicate what kind of review it is\n","\n","vectorizer = TfidfVectorizer(stop_words='english')\n","tfidf = vectorizer.fit_transform(text).toarray()\n","# toarray pads the sequences with zeros to make all sequences of equal length\n","print(\"tfidf  : \", tfidf.shape)\n","\n","# labels for our training data \n","# a score from 1 to 5\n","\n","rating = one_hot(np.asarray(data[\"Rating\"]).reshape(len(data[\"Rating\"]), 1))\n","\n","print(\"rating : \", rating.shape)\n","\n","min_max = MinMaxScaler()\n","vectors = min_max.fit(tfidf)\n","vectors = min_max.transform(tfidf)"],"execution_count":39,"outputs":[{"output_type":"stream","text":["tfidf  :  (22641, 12253)\n","rating :  (22641, 6)\n"],"name":"stdout"}]},{"metadata":{"id":"kZ1eCNLepnXb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":110},"outputId":"5e419845-3f96-4c4f-dee6-708a4b4739c3","executionInfo":{"status":"ok","timestamp":1540878984904,"user_tz":-330,"elapsed":1398,"user":{"displayName":"Aashish Vinayak","photoUrl":"https://lh3.googleusercontent.com/-n6LvT5UqaRw/AAAAAAAAAAI/AAAAAAAAFxw/5OKvaxSzOmE/s64/photo.jpg","userId":"11492081867973563164"}}},"cell_type":"code","source":["# making training and testing data\n","split = int(len(vectors)*0.75)\n","\n","x_train = vectors[0:split]\n","x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n","y_train = data[\"Rating\"][0:split]\n","y_train = to_categorical(y_train.reshape(len(y_train),1), num_classes=6)\n","\n","\n","x_test = vectors[split:len(vectors)]\n","x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n","y_test = data[\"Rating\"][split:len(data[\"Rating\"])]\n","y_test = to_categorical(y_test.reshape(len(y_test),1), num_classes=6)\n","\n","### placeholders\n","x = tf.placeholder(tf.float32, [None, x_train.shape[1], 1])\n","y = tf.placeholder(tf.float32, [None, 6])"],"execution_count":46,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n","  if sys.path[0] == '':\n"],"name":"stderr"}]},{"metadata":{"id":"9DwMCkw-gs8q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":472},"outputId":"6546f410-4060-49ea-f11c-0d919fff15ab","executionInfo":{"status":"ok","timestamp":1540880298587,"user_tz":-330,"elapsed":40913,"user":{"displayName":"Aashish Vinayak","photoUrl":"https://lh3.googleusercontent.com/-n6LvT5UqaRw/AAAAAAAAAAI/AAAAAAAAFxw/5OKvaxSzOmE/s64/photo.jpg","userId":"11492081867973563164"}}},"cell_type":"code","source":["model = Sequential()\n","# bigger kernel to reduce the zeros\n","model.add(Conv1D(32, (4), strides=(1), input_shape=(x_train.shape[1],1), padding='same', activation='relu'))\n","model.add(Conv1D(32, (4), strides=(1), padding='same', activation='relu'))\n","model.add(Conv1D(32, (2), strides=(1), padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=(3), strides=(2), padding=\"same\"))\n","model.add(Dropout(0.75))\n","model.add(Flatten())\n","model.add(Dense(6, activation='softmax'))\n","model.compile(loss = \"categorical_crossentropy\", optimizer=\"nadam\",  metrics = ['accuracy'])\n","model.summary()\n","model.fit(x_train, y_train, epochs=1, batch_size=100)\n","model.evaluate(x_test, y_test)"],"execution_count":66,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv1d_113 (Conv1D)          (None, 12253, 32)         160       \n","_________________________________________________________________\n","conv1d_114 (Conv1D)          (None, 12253, 32)         4128      \n","_________________________________________________________________\n","conv1d_115 (Conv1D)          (None, 12253, 32)         2080      \n","_________________________________________________________________\n","max_pooling1d_37 (MaxPooling (None, 6127, 32)          0         \n","_________________________________________________________________\n","dropout_20 (Dropout)         (None, 6127, 32)          0         \n","_________________________________________________________________\n","flatten_24 (Flatten)         (None, 196064)            0         \n","_________________________________________________________________\n","dense_26 (Dense)             (None, 6)                 1176390   \n","=================================================================\n","Total params: 1,182,758\n","Trainable params: 1,182,758\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/1\n","16980/16980 [==============================] - 34s 2ms/step - loss: 1.0437 - acc: 0.5874\n","5661/5661 [==============================] - 5s 909us/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.9674304510738041, 0.6034269563365459]"]},"metadata":{"tags":[]},"execution_count":66}]},{"metadata":{"id":"beK-PAe-dh-G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":401},"outputId":"c3b145b4-6672-4dea-acdb-e47452e9c089","executionInfo":{"status":"ok","timestamp":1540881023300,"user_tz":-330,"elapsed":1929,"user":{"displayName":"Aashish Vinayak","photoUrl":"https://lh3.googleusercontent.com/-n6LvT5UqaRw/AAAAAAAAAAI/AAAAAAAAFxw/5OKvaxSzOmE/s64/photo.jpg","userId":"11492081867973563164"}}},"cell_type":"code","source":["u = np.random.randint(1000, size=10)\n","\n","for i in u:\n","    print(\"Text   : \", data[\"Review Text\"][i])\n","    sample = x_test[i].reshape(1, x_test[i].shape[0],1)\n","    print(\"Rating : \", np.argmax(model.predict(sample)))"],"execution_count":86,"outputs":[{"output_type":"stream","text":["Text   :  I'm a 110 lb, shorty with a short torso and long arms so the standard xs was huge. i had the opportunity to try on both the xxs petite and xs petite. the xxs petite was almost perfect but i wanted it more flowy - probably best for those that are even shorter and more lightweight than me, othewise, it looks a little too boxy. the petite xs fit me the best and draped nicely, similar to the model in the pink. i had a sweater with this exact silhouette 5+ years ago and wore it to death even though i\n","Rating :  5\n","Text   :  I purchased this blouse because i love a 70's vibe in my tops. it is a beautiful, colorful top, but the colors weren't flattering on me. having said that, the cut is nice, the fabric is lightweight and flows nicely, and the fit was fine on me. i am a curvy 5'5\" with a 36 c cup. go for it if this is a style you like. one other note, i wish it had been a bit longer, but i am older and prefer a little more coverage. it's just a personal preference. i think the picture is an accurate depiction.\n","Rating :  5\n","Text   :  This is a great pull over dress that can easily be dressed to wear to work, to a bbq, or to happy hour. i have a large chest which sometimes makes this fit look like a tent but this fabric drapes nicely. the only thing i would note is the neckline was slightly higher on me than for the model.\n","Rating :  3\n","Text   :  This is a cute top that can transition easily from summer to fall. it fits well, nice print and it's comfortable. i tried this on in the store, but did not purchase it because the color washed me out. this is not the best color for a blonde. would look much better on a brunette. if this was in a different color i most likely would have purchased it.\n","Rating :  5\n","Text   :  I love the fabric and color (i bought the green one). my only complaint is that the base is wider than the picture shows. it looks more fitted on the model. it is more of an a shape (significantly wider at the hips.)\n","Rating :  5\n","Text   :  I really like this blouse a lot. very very easy to wear!! i wore with pencil skirt to work and with skort as shown similar on model with sandals on weekend. very flattering and great blue color!!! very happy with this purchase. highly recommend. i am 5'6\" short torso and my usual 6 worked.\n","Rating :  5\n","Text   :  The horizontal lines on the skirt and top gives the wearer and very nice dress. i am short so i got the 6 petite and it fits perfectly. the fabric is thick and stretchy.\n","Rating :  5\n","Text   :  The silhouette and length of this skirt and length are flattering, classic and comfortable! the colors and weight of this skirt make it versatile - could be worn year-round (so long as it's not 100 degrees out - there is a bit of weight to it). it's one of my favorite pieces in my closet. can be styled 20 different ways. pair with a higher-end tee, tank, denim jacket or body-hugging sweater. i sized down.\n","Rating :  5\n","Text   :  This top has a western look with lots of black and some burgundy shades.  it is very cute and the fabric is soft.\n","Rating :  5\n","Text   :  The top runs small, it is a very sexy and slimming top. i tried in l, it was too small so i have ordered it in xl. i hope it fits because it is very fitted and i am worried that the bottom will roll to the top with a fabric like this. but it is a very sexy, good looking, and slimming top. i love it!!\n","Rating :  5\n"],"name":"stdout"}]},{"metadata":{"id":"aH5urZKbp4Qn","colab_type":"text"},"cell_type":"markdown","source":["# Findings\n","1. CNNs can be used for text classification\n","2. The length of sentences should be nearly the same \n","3. The reason for low accuracy is the zero padding to make sentence length equal, min length of sentence is 2 and max length is 56\n","     this variation in length causes the CNN to saturate at 56% accuracy, this is the major reason"]}]}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_processing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "dMbEn_4Ujtn9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Processing "
      ]
    },
    {
      "metadata": {
        "id": "VvjHybU9224P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "dadc22d6-69b6-4361-b347-f776ed3cf99e"
      },
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "IMT42BOS26ye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "2132cab2-116c-4218-f916-82ade7d43717"
      },
      "cell_type": "code",
      "source": [
        "# opening dataset\n",
        "data = urlopen(\"http://www.gutenberg.org/files/36/36-0.txt\")\n",
        "\n",
        "# list holds all the lines from text\n",
        "text = []\n",
        "\n",
        "for line in data:\n",
        "    text.append(str(line))\n",
        "    \n",
        "text[1:10]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"b'\\\\r\\\\n'\",\n",
              " 'b\"Project Gutenberg\\'s The War of the Worlds, by H. G. Wells\\\\r\\\\n\"',\n",
              " \"b'\\\\r\\\\n'\",\n",
              " \"b'This eBook is for the use of anyone anywhere in the United States and most\\\\r\\\\n'\",\n",
              " \"b'other parts of the world at no cost and with almost no restrictions\\\\r\\\\n'\",\n",
              " \"b'whatsoever. You may copy it, give it away or re-use it under the terms of\\\\r\\\\n'\",\n",
              " \"b'the Project Gutenberg License included with this eBook or online at\\\\r\\\\n'\",\n",
              " 'b\"www.gutenberg.org. If you are not located in the United States, you\\'ll have\\\\r\\\\n\"',\n",
              " \"b'to check the laws of the country where you are located before using this ebook.\\\\r\\\\n'\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "oFqXydc67Yu_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "4d333eb9-208d-4c1e-dac3-be475ced6100"
      },
      "cell_type": "code",
      "source": [
        "# removing unnecessary characters\n",
        "\n",
        "fixed_text = [re.sub(\"[^a-zA-Z]\", \" \", i.lower()) for i in text[100:]]\n",
        "\n",
        "# removing single characters\n",
        "fixed_text = [re.sub(r'\\b\\w{1,1}\\b', '', i) for i in fixed_text]\n",
        "\n",
        "# removing more than 1 spaces by splitting and rejoinig by 1 space \n",
        "fixed_text = [' '.join(fixed_text[i].split()) for i in range(len(fixed_text))]\n",
        "\n",
        "# removing strings with length < 3\n",
        "fixed_text = [i for i in fixed_text if len(i) >3]\n",
        "\n",
        "fixed_text[1:10]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['upon them as impossible or improbable it is curious to recall some of',\n",
              " 'the mental habits of those departed days at most terrestrial men',\n",
              " 'fancied there might be other men upon mars perhaps inferior to',\n",
              " 'themselves and ready to welcome missionary enterprise yet across the',\n",
              " 'gulf of space minds that are to our minds as ours are to those of the',\n",
              " 'beasts that perish intellects vast and cool and unsympathetic',\n",
              " 'regarded this earth with envious eyes and slowly and surely drew their',\n",
              " 'plans against us and early in the twentieth century came the great',\n",
              " 'disillusionment']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "58BX3j0C_JnV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "d8ab1081-9ca4-49d8-f02f-85bce4190518"
      },
      "cell_type": "code",
      "source": [
        "# removing stopwords\n",
        "\n",
        "# importing the stopwords list\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = list(set(stopwords.words(\"english\")))\n",
        "\n",
        "# function for removing stop words\n",
        "def remove_stopwords(text):\n",
        "    text = [word for word in text if not word in stopwords]\n",
        "    return text\n",
        "\n",
        "# removing stop words\n",
        "for i in range(len(fixed_text)):\n",
        "    fixed_text[i] = remove_stopwords(fixed_text[i].split())\n",
        "    \n",
        "fixed_text[1:10]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['upon', 'impossible', 'improbable', 'curious', 'recall'],\n",
              " ['mental', 'habits', 'departed', 'days', 'terrestrial', 'men'],\n",
              " ['fancied', 'might', 'men', 'upon', 'mars', 'perhaps', 'inferior'],\n",
              " ['ready', 'welcome', 'missionary', 'enterprise', 'yet', 'across'],\n",
              " ['gulf', 'space', 'minds', 'minds'],\n",
              " ['beasts', 'perish', 'intellects', 'vast', 'cool', 'unsympathetic'],\n",
              " ['regarded', 'earth', 'envious', 'eyes', 'slowly', 'surely', 'drew'],\n",
              " ['plans', 'us', 'early', 'twentieth', 'century', 'came', 'great'],\n",
              " ['disillusionment']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "oviuhljeCT0G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "727b73c8-d6af-4d1a-9423-704ad2de8ab1"
      },
      "cell_type": "code",
      "source": [
        "# creating vocabulary\n",
        "vocab = list(set([word for line in fixed_text for word in line]))\n",
        "# sorting alphabetically\n",
        "vocab = sorted(vocab)\n",
        "\n",
        "# appending token for null word, i.e., word for when there is no actual word in the word window\n",
        "vocab.append(\"<null>\")\n",
        "\n",
        "# creating a word to index dictionary; will be useful in one_hot encoding\n",
        "word_to_id = {word:i for i, word in enumerate(vocab)}\n",
        "\n",
        "# creating a index to word dictionary; will be useful in one_hot decoding\n",
        "id_to_word = {i:word for i, word in enumerate(vocab)}\n",
        "\n",
        "## Summary\n",
        "print(\"Summary\")\n",
        "print(\"No. of Sentences: \", len(fixed_text))\n",
        "print(\"vocab length: \", len(vocab))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summary\n",
            "No. of Sentences:  5546\n",
            "vocab length:  7013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e61_gHy4F-uW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# function for one hot encoding\n",
        "def one_hot(text, vocab):\n",
        "    hot_matrix = np.zeros([len(text), len(vocab)])\n",
        "    for i in range(len(text)):\n",
        "        if text[i] in word_to_id:\n",
        "            hot_matrix[i, word_to_id[text[i]]] = 1\n",
        "\n",
        "    return hot_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G9xC79PlOz-2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_word_window(line):\n",
        "    \"\"\"\n",
        "        line: a single sentence or list of words\n",
        "        \n",
        "        append words in the format [ center word, left word, right word] \n",
        "    \"\"\"    \n",
        "    \n",
        "    # empty list for each word window\n",
        "    window = []\n",
        "    \n",
        "    # loop over the sentence\n",
        "    for i in range(len(line)):\n",
        "        \n",
        "        # if there's nothing on the left, <null> tag will be appended\n",
        "        if i == 0 and i != len(line)-1:\n",
        "            left = \"<null>\"\n",
        "            right = line[i+1]\n",
        "            window.append([line[i], left, right])\n",
        "\n",
        "        elif i == 0 and i == len(line)-1:\n",
        "            left = \"<null>\"\n",
        "            right = \"<null>\"\n",
        "            window.append([line[i], left, right])\n",
        "            \n",
        "        elif i == len(line)-1:\n",
        "            left = line[i-1]\n",
        "            right = \"<null>\"\n",
        "            window.append([line[i], left, right])\n",
        "        else: \n",
        "            left = line[i-1]\n",
        "            right = line[i+1]\n",
        "            window.append([line[i], left, right])\n",
        "            \n",
        "    return window"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "11GJPFLieEeq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# list of word windows\n",
        "\n",
        "final_text = []\n",
        "\n",
        "for i in fixed_text:\n",
        "    w_w = create_word_window(i)\n",
        "    final_text = final_text + w_w\n",
        "    \n",
        "# converting to one hot\n",
        "final_text = [one_hot(line, vocab) for line in final_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NY6IEMGYht-i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3e4b376e-cd23-4932-d419-5c5ba2f583af"
      },
      "cell_type": "code",
      "source": [
        "final_text_one_hot = np.asarray(final_text)\n",
        "\n",
        "print(\"final one hot vector: \",final_text_one_hot[0].shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33205, 3, 7013)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}